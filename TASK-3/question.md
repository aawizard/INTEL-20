## Task 3: Data Cleaning and Data Analysis

1. Clean the training dataset `train_set.txt` and prepare it for tokenization.
2. Tokenize the dataset and save the tokenized data as a separate csv.
3. Use a clustering algorithm (K means is an example) and find out how many clusters you can make out of the data, and how distinct these clusters are. Optimize your algorithm to have as distinct clusters as possible, while also having as few clusters (since having a thousand clusters in a dataset containing a thousand data points is not very useful for data analysis).


### Instructions
- Writing the code is easy: you need to explain the reasons behind the code you wrote and the decisions you made.
- You must build upon your previous work for this task. You shouldn't use the csv files or the python files other participants have made.
- Create a folder inside this one with your github username and put your files in there.
- Write modularized code: don't throw everything into one giant python file.
- Please comment your code so everyone who knows python can understand what your code is doing.

You need to follow PEP8 guidelines when writing the code. I recommend using `flake8`.
